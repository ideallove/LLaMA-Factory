# Note: DO NOT use quantized model or quantization_bit when merging lora adapters

# CUDA_VISIBLE_DEVICES=1 llamafactory-cli export ./my/sft_lora_merge.yaml

# model
model_name_or_path: /home/lx/models/Meta-Llama-3-8B-Instruct
adapter_name_or_path: /home/lx/models/sft/rog_240519
template: llama3_self
finetuning_type: lora

additional_target: embed_tokens,lm_head,norm
new_special_tokens: "<PATH>, </PATH>, <SEP>, <PAD>"
resize_vocab: true
split_special_tokens: true
use_fast_tokenizer: false
print_param_status: true

# export
export_dir: /home/lx/models/rog_240519_merged
export_size: 10
export_device: cuda
export_legacy_format: true

