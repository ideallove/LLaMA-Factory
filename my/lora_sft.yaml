# CUDA_VISIBLE_DEVICES=0 llamafactory-cli train my/lora_sft.yaml

# model
model_name_or_path: /home/dyf/model/Llama-2-7b-chat-hf-modified

# method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
packing: false
upcast_layernorm: true

# train
max_grad_norm: 1.0
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-05
num_train_epochs: 3.0
lr_scheduler_type: cosine
#warmup_steps: 0.1
weight_decay: 0.0
warmup_ratio: 0.03
fp16: true
#bf16: true
optim: adamw_torch
use_fast_tokenizer: false
resize_vocab: true
split_special_tokens: true \
additional_target: embed_tokens,lm_head,norm
use_cache: false
#flash_attn: fa2

# dataset
dataset: rog_train_all_shuffled
template: llama2_self
cutoff_len: 2048
max_samples: 118000
#val_size: 0.1
overwrite_cache: true
preprocessing_num_workers: 1

# output
output_dir: /home/finetune/llama2-7b-lora-sft
logging_steps: 10
save_steps: 500
save_total_limit: 10
plot_loss: true
overwrite_output_dir: true
report_to: none

# eval
#per_device_eval_batch_size: 1
#evaluation_strategy: steps
#eval_steps: 500
