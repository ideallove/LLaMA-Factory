# 启动指令
# llamafactory-cli train ./my/train.yaml
# OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train ./my/train.yaml
# OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=0 llamafactory-cli train ./my/train.yaml
# pip install -e '.[torch,metrics]'
# Meta-Llama-3-8B-Instruct
# Mistral-7B-Instruct-v0.3
# Qwen2-7B-Instruct

# user-defined
model_name_or_path: /home/lx/models/Meta-Llama-3-8B-Instruct
output_dir: /home/lx/models/sft/rog_llama3_240613
run_name: rog_llama3_240613
template: llama3_self
dataset: rog_train_shuffled_new
cutoff_len: 1536
max_samples: 150000

# method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
#additional_target: embed_tokens, lm_head, norm
#new_special_tokens: "<PATH>, </PATH>, <SEP>, <PAD>"
#flash_attn: fa2
#use_unsloth: true

# ddp
ddp_timeout: 180000000
ddp_find_unused_parameters: false

# dataset
#val_size: 0.1
overwrite_cache: true
preprocessing_num_workers: 1

# output
log_level: debug
logging_steps: 10
save_steps: 500
save_total_limit: 10
plot_loss: true
#overwrite_output_dir: true
report_to: none

# train
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2.0e-5
num_train_epochs: 3.0
lr_scheduler_type: cosine
weight_decay: 0.0
warmup_ratio: 0.03
#fp16: true
bf16: true
#max_grad_norm: 1.0
#optim: adamw_torch
#use_fast_tokenizer: false
#split_special_tokens: false
#resize_vocab: true
#use_cache: false

# eval
#per_device_eval_batch_size: 1
#evaluation_strategy: steps
#eval_steps: 500
