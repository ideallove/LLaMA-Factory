# 启动指令
# CUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train ./my/train.yaml
# CUDA_VISIBLE_DEVICES=0 llamafactory-cli train ./my/train.yaml

# model
model_name_or_path: /home/lx/models/Llama-2-7b-chat-hf
output_dir: /home/lx/models/sft/lmgr_240604
run_name: lmgr_240604

# method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
#new_special_tokens: "<PATH>, </PATH>, <SEP>, <PAD>"
additional_target: embed_tokens, lm_head, norm
#flash_attn: fa2
#use_unsloth: true

# ddp
ddp_timeout: 180000000
#deepspeed: my/ds_z3_config.json

# dataset
dataset: rog_train_all_shuffled
template: llama2_self
cutoff_len: 2048
max_samples: 130000
#val_size: 0.1
overwrite_cache: true
preprocessing_num_workers: 1

# output
log_level: debug
logging_steps: 20
save_steps: 1000
save_total_limit: 10
plot_loss: true
#overwrite_output_dir: true
report_to: wandb

# train
per_device_train_batch_size: 1
gradient_accumulation_steps: 10
learning_rate: 0.00002
num_train_epochs: 3.0
lr_scheduler_type: cosine
weight_decay: 0.0
warmup_ratio: 0.03
#warmup_steps: 0.1 # warmup_ratio has higher priority
#fp16: true
bf16: true
max_grad_norm: 1.0
optim: adamw_torch
use_fast_tokenizer: false
split_special_tokens: false
resize_vocab: true
use_cache: false

# eval
#per_device_eval_batch_size: 1
#evaluation_strategy: steps
#eval_steps: 500
