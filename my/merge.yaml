# Note: DO NOT use quantized model or quantization_bit when merging lora adapters

# CUDA_VISIBLE_DEVICES=1 llamafactory-cli export ./my/merge.yaml
# Llama-2-7b-chat-hf

# model
model_name_or_path: /home/lx/models/Meta-Llama-3-8B-Instruct
adapter_name_or_path: /home/lx/models/sft/rog_llama3_240612
export_dir: /home/lx/models/rog_llama3_240612_merged
template: llama3_self

finetuning_type: lora

#additional_target: embed_tokens,lm_head,norm
#new_special_tokens: "<PATH>, </PATH>, <SEP>, <PAD>"
#resize_vocab: true
#split_special_tokens: false
#use_fast_tokenizer: false
#use_cache: false
#print_param_status: true

# export
export_size: 5
export_device: cuda
export_legacy_format: false
